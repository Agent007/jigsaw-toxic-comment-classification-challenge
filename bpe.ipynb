{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# train = train.sample(frac=0.1)  # 157975 original total, so let's prototype models with a fraction of that\n",
    "validation_fraction = 0.1  # change to 1% for training on complete training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n",
      "159571\n",
      "153164\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "train[\"comment_text\"].fillna(\"fillna\")\n",
    "test[\"comment_text\"].fillna(\"fillna\")\n",
    "\n",
    "X_train = train[\"comment_text\"].str.lower()\n",
    "print(len(X_train))\n",
    "# X_train.to_csv(\"X_train.csv\", index=False)\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "print(len(y_train))\n",
    "\n",
    "X_test = test[\"comment_text\"].str.lower()\n",
    "# X_test.to_csv(\"X_test.csv\", index=False)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    explanation\\nwhy the edits made under my usern...\n",
       "1    d'aww! he matches this background colour i'm s...\n",
       "2    hey man, i'm really not trying to edit war. it...\n",
       "3    \"\\nmore\\ni can't make any real suggestions on ...\n",
       "4    you, sir, are my hero. any chance you remember...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"data\"\n",
    "training_directory = data_directory + \"/train\"\n",
    "test_directory = data_directory + \"/test\"\n",
    "\n",
    "! mkdir -p {training_directory}\n",
    "! mkdir -p {test_directory}\n",
    "\n",
    "for index, row in train[[\"id\", \"comment_text\"]].iterrows():\n",
    "    filename = row[\"id\"] + \".comment.txt\"\n",
    "    with open(training_directory + \"/\" + filename, \"w\") as file:\n",
    "        file.write(row[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in test[[\"id\", \"comment_text\"]].iterrows():\n",
    "    filename = row[\"id\"] + \".comment.txt\"\n",
    "    with open(test_directory + \"/\" + filename, \"w\") as file:\n",
    "        file.write(row[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -f {training_directory}/*.clean\n",
    "! rm -f {test_directory}/*.clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "for file in data/train/*.comment.txt\n",
    "do\n",
    "    ./preprocess_text.sh ${file}\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for file in data/test/*.comment.txt\n",
    "do\n",
    "    ./preprocess_text.sh ${file}\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\r\n"
     ]
    }
   ],
   "source": [
    "! find -type f -wholename './{training_directory}/*.clean' | wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164\r\n"
     ]
    }
   ],
   "source": [
    "! find -type f -wholename './{test_directory}/*.clean' | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d'aww! he matches this background colour i'm s...</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man, i'm really not trying to edit war. it...</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nmore\\ni can't make any real suggestions on ...</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you, sir, are my hero. any chance you remember...</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text                id\n",
       "0  explanation\\nwhy the edits made under my usern...  0000997932d777bf\n",
       "1  d'aww! he matches this background colour i'm s...  000103f0d9cfb60f\n",
       "2  hey man, i'm really not trying to edit war. it...  000113f07ec002fd\n",
       "3  \"\\nmore\\ni can't make any real suggestions on ...  0001b41b1c6bb37e\n",
       "4  you, sir, are my hero. any chance you remember...  0001d958c54c6e35"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bpe_preprocessed = []\n",
    "for index, row in train[[\"id\"]].iterrows():\n",
    "    filename = row[\"id\"] + \".comment.txt.clean\"\n",
    "    with open(training_directory + \"/\" + filename, \"r\") as file:\n",
    "        comment_text = file.read()\n",
    "        train_bpe_preprocessed.append({\"id\" : row[\"id\"], \"comment_text\" : comment_text})\n",
    "\n",
    "train_bpe_preprocessed_df = pd.DataFrame.from_records(train_bpe_preprocessed)\n",
    "train_bpe_preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_bpe_preprocessed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164\n"
     ]
    }
   ],
   "source": [
    "test_bpe_preprocessed = []\n",
    "for index, row in test[[\"id\"]].iterrows():\n",
    "    filename = row[\"id\"] + \".comment.txt.clean\"\n",
    "    with open(test_directory + \"/\" + filename, \"r\") as file:\n",
    "        comment_text = file.read()\n",
    "        test_bpe_preprocessed.append({\"id\" : row[\"id\"], \"comment_text\" : comment_text})\n",
    "\n",
    "test_bpe_preprocessed_df = pd.DataFrame.from_records(test_bpe_preprocessed)\n",
    "test_bpe_preprocessed_df.head()\n",
    "print(len(test_bpe_preprocessed_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    explanation\\nwhy the edits made under my usern...\n",
       "1    d'aww! he matches this background colour i'm s...\n",
       "2    hey man, i'm really not trying to edit war. it...\n",
       "3    \"\\nmore\\ni can't make any real suggestions on ...\n",
       "4    you, sir, are my hero. any chance you remember...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bpe_preprocessed = train_bpe_preprocessed_df[\"comment_text\"]\n",
    "X_train_bpe_preprocessed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bpe_preprocessed.to_csv(\"X_train.clean\", index=False)\n",
    "\n",
    "X_test_bpe_preprocessed = test_bpe_preprocessed_df[\"comment_text\"]\n",
    "X_test_bpe_preprocessed.to_csv(\"X_test.clean\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n",
      "153164\n"
     ]
    }
   ],
   "source": [
    "X_train_bpe_preprocessed = pd.read_csv(\"X_train.clean\", header=None, names=[\"comment_text\"])[\"comment_text\"]\n",
    "X_test_bpe_preprocessed = pd.read_csv(\"X_test.clean\", header=None, names=[\"comment_text\"])[\"comment_text\"]\n",
    "\n",
    "print(len(X_train_bpe_preprocessed))\n",
    "print(len(X_test_bpe_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    explanation\\nwhy the edits made under my usern...\n",
       "1    d'aww! he matches this background colour i'm s...\n",
       "2    hey man, i'm really not trying to edit war. it...\n",
       "3    \"\\nmore\\ni can't make any real suggestions on ...\n",
       "4    you, sir, are my hero. any chance you remember...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bpe_preprocessed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 780 ms, sys: 58.9 ms, total: 838 ms\n",
      "Wall time: 859 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "max_features = None  # 30000\n",
    "embed_size = 300  # should match embedding file\n",
    "\n",
    "# tokenizer = text.Tokenizer(num_words=max_features)\n",
    "# all_comments = list(X_train) + list(X_test)\n",
    "# tokenizer.fit_on_texts(all_comments)\n",
    "# X_train_tokenized = tokenizer.texts_to_sequences(X_train)\n",
    "# X_test_tokenized = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "class BPETokenizer(text.Tokenizer):\n",
    "    \"\"\"Text tokenization utility class.\n",
    "    This class allows to vectorize a text corpus, by turning each\n",
    "    text into either a sequence of integers (each integer being the index\n",
    "    of a token in a dictionary) or into a vector where the coefficient\n",
    "    for each token could be binary, based on word count, based on tf-idf...\n",
    "    # Arguments\n",
    "        num_words: the maximum number of words to keep, based\n",
    "            on word frequency. Only the most common `num_words` words will\n",
    "            be kept.\n",
    "        filters: a string where each element is a character that will be\n",
    "            filtered from the texts. The default is all punctuation, plus\n",
    "            tabs and line breaks, minus the `'` character.\n",
    "        lower: boolean. Whether to convert the texts to lowercase.\n",
    "        split: character or string to use for token splitting.\n",
    "        char_level: if True, every character will be treated as a token.\n",
    "        oov_token: if given, it will be added to word_index and used to\n",
    "            replace out-of-vocabulary words during text_to_sequence calls\n",
    "    By default, all punctuation is removed, turning the texts into\n",
    "    space-separated sequences of words\n",
    "    (words maybe include the `'` character). These sequences are then\n",
    "    split into lists of tokens. They will then be indexed or vectorized.\n",
    "    `0` is a reserved index that won't be assigned to any word.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_words=None,\n",
    "                 filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                 lower=True,\n",
    "                 split=' ',\n",
    "                 char_level=False,\n",
    "                 oov_token=None,\n",
    "                 **kwargs):\n",
    "        # Legacy support\n",
    "        if 'nb_words' in kwargs:\n",
    "            warnings.warn('The `nb_words` argument in `Tokenizer` '\n",
    "                          'has been renamed `num_words`.')\n",
    "            num_words = kwargs.pop('nb_words')\n",
    "        if kwargs:\n",
    "            raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n",
    "        \n",
    "        self.word_counts = OrderedDict()\n",
    "        self.word_docs = {}\n",
    "        self.filters = filters\n",
    "        self.split = split\n",
    "        self.lower = lower\n",
    "        self.num_words = num_words\n",
    "        self.document_count = 0\n",
    "        self.char_level = char_level\n",
    "        self.oov_token = oov_token\n",
    "        self.index_docs = {}\n",
    "        \n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.Load(\"en.wiki.bpe.op200000.model\")\n",
    "    \n",
    "    def text_to_word_sequence(self, \n",
    "                              text,\n",
    "                              filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                              lower=True, \n",
    "                              split=\" \"):\n",
    "        \"\"\"Converts a text to a sequence of words (or tokens).\n",
    "        # Arguments\n",
    "            text: Input text (string).\n",
    "            filters: Sequence of characters to filter out.\n",
    "            lower: Whether to convert the input to lowercase. (Unused here since input text should already be lowercased)\n",
    "            split: Sentence split marker (string).\n",
    "        # Returns\n",
    "            A list of words (or tokens).\n",
    "        \"\"\"\n",
    "        return self.sp.EncodeAsPieces(text)\n",
    "    \n",
    "    def fit_on_texts(self, texts):\n",
    "        \"\"\"Updates internal vocabulary based on a list of texts.\n",
    "        In the case where texts contains lists, we assume each entry of the lists\n",
    "        to be a token.\n",
    "        Required before using `texts_to_sequences` or `texts_to_matrix`.\n",
    "        # Arguments\n",
    "            texts: can be a list of strings,\n",
    "                a generator of strings (for memory-efficiency),\n",
    "                or a list of list of strings.\n",
    "        \"\"\"\n",
    "        for text in texts:\n",
    "            self.document_count += 1\n",
    "            if self.char_level or isinstance(text, list):\n",
    "                seq = text\n",
    "            else:\n",
    "                seq = self.text_to_word_sequence(text,\n",
    "                                            self.filters,\n",
    "                                            self.lower,\n",
    "                                            self.split)\n",
    "            for w in seq:\n",
    "                if w in self.word_counts:\n",
    "                    self.word_counts[w] += 1\n",
    "                else:\n",
    "                    self.word_counts[w] = 1\n",
    "            for w in set(seq):\n",
    "                if w in self.word_docs:\n",
    "                    self.word_docs[w] += 1\n",
    "                else:\n",
    "                    self.word_docs[w] = 1\n",
    "        \n",
    "        wcounts = list(self.word_counts.items())\n",
    "        wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "        sorted_voc = [wc[0] for wc in wcounts]\n",
    "        # note that index 0 is reserved, never assigned to an existing word\n",
    "        self.word_index = dict(list(zip(sorted_voc, list(range(1, len(sorted_voc) + 1)))))\n",
    "        \n",
    "        if self.oov_token is not None:\n",
    "            i = self.word_index.get(self.oov_token)\n",
    "            if i is None:\n",
    "                self.word_index[self.oov_token] = len(self.word_index) + 1\n",
    "        \n",
    "        for w, c in list(self.word_docs.items()):\n",
    "            self.index_docs[self.word_index[w]] = c\n",
    "\n",
    "bpe_tokenizer = BPETokenizer(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_bpe_preprocessed = list(X_train_bpe_preprocessed) + list(X_test_bpe_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer.fit_on_texts(all_comments_bpe_preprocessed)\n",
    "X_train_tokenized_bpe = bpe_tokenizer.texts_to_sequences(X_train_bpe_preprocessed)\n",
    "X_test_tokenized_bpe = bpe_tokenizer.texts_to_sequences(X_test_bpe_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 300  # 150  # 128  # 100, 200, 256 worsened validation AUC score  # 100\n",
    "\n",
    "X_train_padded_bpe = sequence.pad_sequences(X_train_tokenized_bpe, maxlen=max_sequence_length)\n",
    "X_test_padded_bpe = sequence.pad_sequences(X_test_tokenized_bpe, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bpe_word_index = bpe_tokenizer.word_index  # len(bpe_word_index) == 162739\n",
    "word_count = min(max_features, len(bpe_word_index)) if max_features else len(bpe_word_index)\n",
    "# embedding_matrix = np.random.uniform(-1.0, 1.0, (nb_words, embed_size))  # in case you don't want to use pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162739"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bpe_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 330 ms, sys: 16 ms, total: 346 ms\n",
      "Wall time: 368 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import bcolz\n",
    "\n",
    "\n",
    "def process_fasttext_line(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(matrix, embeddings_index, word_index):\n",
    "    for word, i in word_index.items():\n",
    "        if max_features and i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            matrix[i] = embedding_vector\n",
    "    return matrix\n",
    "\n",
    "def build_embedding_matrix(matrix, bcolz_rootdir, embeddings_filename, line_processing_function, word_index):\n",
    "    try:\n",
    "        matrix = bcolz.open(rootdir=bcolz_rootdir)\n",
    "    except FileNotFoundError:\n",
    "        embeddings_index = dict(line_processing_function(*line.rstrip().rsplit()) for line in open(embeddings_filename))\n",
    "        matrix = load_embeddings(matrix, embeddings_index, word_index)\n",
    "        matrix = bcolz.carray(matrix, rootdir=bcolz_rootdir)\n",
    "        matrix.flush()\n",
    "    return matrix\n",
    "\n",
    "bpe_embedding_matrix = np.zeros((word_count + 1, embed_size))\n",
    "bpe_embedding_matrix = build_embedding_matrix(bpe_embedding_matrix, \"en.wiki.bpe.op200000.d300.w2v.bcolz\", \"en.wiki.bpe.op200000.d300.w2v.txt\", process_fasttext_line, bpe_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation, BatchNormalization, Bidirectional, concatenate, Conv1D, CuDNNGRU, Dense, Dropout, Embedding, Flatten, Input, GlobalAveragePooling1D, GlobalMaxPooling1D, GRU, K, SpatialDropout1D\n",
    "from keras.optimizers import Adam, Nadam\n",
    "# from keras.regularizers import l2\n",
    "# from keras.constraints import maxnorm\n",
    "\n",
    "# from qrnn import QRNN\n",
    "# from attention import AttentionWithContext\n",
    "from capsnet import Capsule\n",
    "\n",
    "def build_model(max_sequence_length, word_count, embed_size, embeddings, spatial_dropout=0.28, dropout=0.25):\n",
    "    i = Input(shape=(max_sequence_length, ))\n",
    "    # fasttext = Embedding(word_count + 1, embed_size, weights=[embeddings[\"fasttext\"][:word_count + 2, ]], trainable=False)(i)\n",
    "    # numberbatch = Embedding(word_count + 1, embed_size, weights=[embeddings[\"numberbatch\"][:word_count + 2, ]], trainable=False)(i)\n",
    "    # glove = Embedding(word_count + 1, embed_size, weights=[embeddings[\"glove\"][:word_count + 2, ]], trainable=False)(i)\n",
    "    bpe = fasttext = Embedding(word_count + 1, embed_size, weights=[embeddings[\"bpe\"][:word_count + 2, ]], trainable=False)(i)\n",
    "    # fasttext = SpatialDropout1D(spatial_dropout)(fasttext)\n",
    "    # numberbatch = SpatialDropout1D(spatial_dropout)(numberbatch)\n",
    "    # glove = SpatialDropout1D(spatial_dropout)(glove)\n",
    "    bpe = SpatialDropout1D(spatial_dropout)(bpe)\n",
    "    rnn_size = 128  # max_sequence_length  # 140\n",
    "    # x = Bidirectional(CuDNNGRU(rnn_size, return_sequences=True))(x)  # 2nd bidirectional layer didn't help with training subsample\n",
    "    # x = QRNN(rnn_size, window_size=7, return_sequences=True)(x)\n",
    "    # x = Bidirectional(CuDNNGRU(rnn_size, return_sequences=True))(x)\n",
    "    # x = Bidirectional(GRU(rnn_size, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "    # x = Bidirectional(GRU(64, return_sequences=True,dropout=0.3,recurrent_dropout=0.3))(x)\n",
    "    # x = Bidirectional(GRU(rnn_size, activation='relu', return_sequences=True, dropout=dropout, recurrent_dropout=dropout))(x)\n",
    "    # fasttext = Bidirectional(GRU(rnn_size, activation='relu', return_sequences=True, dropout=dropout, recurrent_dropout=dropout))(fasttext)\n",
    "    # numberbatch = Bidirectional(GRU(rnn_size, activation='relu', return_sequences=True, dropout=dropout, recurrent_dropout=dropout))(numberbatch)\n",
    "    # glove = Bidirectional(GRU(rnn_size, activation='relu', return_sequences=True, dropout=dropout, recurrent_dropout=dropout))(glove)\n",
    "    bpe = Bidirectional(GRU(rnn_size, activation='relu', return_sequences=True, dropout=dropout, recurrent_dropout=dropout))(bpe)\n",
    "    # fasttext = Capsule()(fasttext)\n",
    "    # numberbatch = Capsule()(numberbatch)\n",
    "    # glove = Capsule()(glove)\n",
    "    bpe = Capsule()(bpe)\n",
    "    # attention = AttentionWithContext()(x)\n",
    "    # x = concatenate([\n",
    "    #     fasttext, \n",
    "    #     numberbatch, \n",
    "        # glove\n",
    "    # ])\n",
    "    x = bpe\n",
    "    x = Flatten()(x)\n",
    "    d = Dropout(dropout)(x)\n",
    "    multiclass_label_count = 6\n",
    "    out = Dense(multiclass_label_count, activation=\"sigmoid\")(d)\n",
    "    model = Model(inputs=i, outputs=out)\n",
    "    optimizer = \"adam\"  # Nadam(lr=1e-3)  # 'nadam'  # Nadam(lr=1e-5)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# del model\n",
    "embeddings = { # \"fasttext\" : fasttext_embedding_matrix, \n",
    "              # \"numberbatch\" : numberbatch_embedding_matrix,\n",
    "              # \"glove\" : glove_embedding_matrix\n",
    "              \"bpe\" : bpe_embedding_matrix\n",
    "             }\n",
    "# model = build_model(max_sequence_length, word_count, embed_size, embeddings)\n",
    "# del models\n",
    "# models = [fasttext_model, numberbatch_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# X_train_split, X_val, y_train_split, y_val = train_test_split(X_train_padded, y_train, test_size=validation_fraction)\n",
    "\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    \"\"\"https://www.kaggle.com/demesgal/lstm-glove-lr-decrease-bn-cv-lb-0-047/comments\"\"\"\n",
    "    \n",
    "    def __init__(self, validation_data=(), max_epoch=20, cross_validation_fold=None):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.stopped_epoch = max_epoch\n",
    "        self.best = 0.0\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.y_pred = np.zeros(self.y_val.shape)\n",
    "        self.cross_validation_fold = cross_validation_fold  # integer\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.X_val, verbose=1)\n",
    "        \"\"\"Important lines\"\"\"\n",
    "        current = roc_auc_score(self.y_val, y_pred)\n",
    "        logs['val_auc'] = current\n",
    "\n",
    "        if current > self.best:  # save model\n",
    "            self.best = current\n",
    "            self.y_pred = y_pred\n",
    "            self.stopped_epoch = epoch + 1\n",
    "            filename = \"bpe.weights.{fold:02d}-{epoch:02d}-{val_auc:.4f}.hdf5\".format(fold=self.cross_validation_fold, epoch=(epoch + 1), val_auc=current) if self.cross_validation_fold is not None else \"bpe.weights.{epoch:02d}-{val_auc:.4f}.hdf5\".format(epoch=(epoch + 1), val_auc=current) \n",
    "            print(\"saving \" + filename)\n",
    "            self.model.save_weights(filename, overwrite=True)\n",
    "\n",
    "        print(\"val_auc: {:.4f}\".format(current))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/16\n",
      "143613/143613 [==============================] - 661s 5ms/step - loss: 0.0966 - acc: 0.9725 - val_loss: 0.0659 - val_acc: 0.9775\n",
      "15958/15958 [==============================] - 64s 4ms/step\n",
      "saving bpe.weights.00-01-0.9478.hdf5\n",
      "val_auc: 0.9478\n",
      "Epoch 2/16\n",
      "143613/143613 [==============================] - 333s 2ms/step - loss: 0.0662 - acc: 0.9779 - val_loss: 0.0592 - val_acc: 0.9796\n",
      "15958/15958 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.00-02-0.9654.hdf5\n",
      "val_auc: 0.9654\n",
      "Epoch 3/16\n",
      "143613/143613 [==============================] - 337s 2ms/step - loss: 0.0619 - acc: 0.9789 - val_loss: 0.0582 - val_acc: 0.9796\n",
      "15958/15958 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.00-03-0.9687.hdf5\n",
      "val_auc: 0.9687\n",
      "Epoch 4/16\n",
      "143613/143613 [==============================] - 334s 2ms/step - loss: 0.0600 - acc: 0.9794 - val_loss: 0.0575 - val_acc: 0.9802\n",
      "15958/15958 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.00-04-0.9693.hdf5\n",
      "val_auc: 0.9693\n",
      "Epoch 5/16\n",
      "143613/143613 [==============================] - 338s 2ms/step - loss: 0.0579 - acc: 0.9798 - val_loss: 0.0552 - val_acc: 0.9805\n",
      "15958/15958 [==============================] - 62s 4ms/step\n",
      "saving bpe.weights.00-05-0.9731.hdf5\n",
      "val_auc: 0.9731\n",
      "Epoch 6/16\n",
      "143613/143613 [==============================] - 335s 2ms/step - loss: 0.0562 - acc: 0.9803 - val_loss: 0.0550 - val_acc: 0.9806\n",
      "15958/15958 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.00-06-0.9740.hdf5\n",
      "val_auc: 0.9740\n",
      "Epoch 7/16\n",
      "143613/143613 [==============================] - 336s 2ms/step - loss: 0.0547 - acc: 0.9806 - val_loss: 0.0537 - val_acc: 0.9809\n",
      "15958/15958 [==============================] - 62s 4ms/step\n",
      "saving bpe.weights.00-07-0.9750.hdf5\n",
      "val_auc: 0.9750\n",
      "Epoch 8/16\n",
      "143613/143613 [==============================] - 337s 2ms/step - loss: 0.0532 - acc: 0.9810 - val_loss: 0.0543 - val_acc: 0.9807\n",
      "15958/15958 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.00-08-0.9754.hdf5\n",
      "val_auc: 0.9754\n",
      "Epoch 9/16\n",
      "143613/143613 [==============================] - 335s 2ms/step - loss: 0.0526 - acc: 0.9813 - val_loss: 0.0532 - val_acc: 0.9809\n",
      "15958/15958 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.00-09-0.9765.hdf5\n",
      "val_auc: 0.9765\n",
      "Epoch 10/16\n",
      "143613/143613 [==============================] - 339s 2ms/step - loss: 0.0519 - acc: 0.9815 - val_loss: 0.0531 - val_acc: 0.9812\n",
      "15958/15958 [==============================] - 63s 4ms/step\n",
      "val_auc: 0.9762\n",
      "Epoch 11/16\n",
      "143613/143613 [==============================] - 334s 2ms/step - loss: 0.0508 - acc: 0.9817 - val_loss: 0.0536 - val_acc: 0.9804\n",
      "15958/15958 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.00-11-0.9768.hdf5\n",
      "val_auc: 0.9768\n",
      "Epoch 12/16\n",
      "143613/143613 [==============================] - 339s 2ms/step - loss: 0.0500 - acc: 0.9820 - val_loss: 0.0532 - val_acc: 0.9809\n",
      "15958/15958 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.00-12-0.9768.hdf5\n",
      "val_auc: 0.9768\n",
      "Epoch 13/16\n",
      "143613/143613 [==============================] - 335s 2ms/step - loss: 0.0491 - acc: 0.9822 - val_loss: 0.0535 - val_acc: 0.9805\n",
      "15958/15958 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.00-13-0.9773.hdf5\n",
      "val_auc: 0.9773\n",
      "Epoch 14/16\n",
      "143613/143613 [==============================] - 338s 2ms/step - loss: 0.0487 - acc: 0.9823 - val_loss: 0.0549 - val_acc: 0.9798\n",
      "15958/15958 [==============================] - 63s 4ms/step\n",
      "val_auc: 0.9768\n",
      "Epoch 15/16\n",
      "143613/143613 [==============================] - 335s 2ms/step - loss: 0.0483 - acc: 0.9824 - val_loss: 0.0525 - val_acc: 0.9810\n",
      "15958/15958 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.00-15-0.9778.hdf5\n",
      "val_auc: 0.9778\n",
      "Epoch 16/16\n",
      "143613/143613 [==============================] - 339s 2ms/step - loss: 0.0474 - acc: 0.9828 - val_loss: 0.0545 - val_acc: 0.9800\n",
      "15958/15958 [==============================] - 63s 4ms/step\n",
      "val_auc: 0.9772\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/16\n",
      "143614/143614 [==============================] - 338s 2ms/step - loss: 0.0989 - acc: 0.9720 - val_loss: 0.0679 - val_acc: 0.9769\n",
      "15957/15957 [==============================] - 64s 4ms/step\n",
      "saving bpe.weights.01-01-0.9547.hdf5\n",
      "val_auc: 0.9547\n",
      "Epoch 2/16\n",
      "143614/143614 [==============================] - 340s 2ms/step - loss: 0.0656 - acc: 0.9784 - val_loss: 0.0614 - val_acc: 0.9789\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.01-02-0.9649.hdf5\n",
      "val_auc: 0.9649\n",
      "Epoch 3/16\n",
      "143614/143614 [==============================] - 337s 2ms/step - loss: 0.0611 - acc: 0.9794 - val_loss: 0.0578 - val_acc: 0.9798\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.01-03-0.9707.hdf5\n",
      "val_auc: 0.9707\n",
      "Epoch 4/16\n",
      "143614/143614 [==============================] - 339s 2ms/step - loss: 0.0578 - acc: 0.9801 - val_loss: 0.0586 - val_acc: 0.9790\n",
      "15957/15957 [==============================] - 62s 4ms/step\n",
      "saving bpe.weights.01-04-0.9731.hdf5\n",
      "val_auc: 0.9731\n",
      "Epoch 5/16\n",
      "143614/143614 [==============================] - 339s 2ms/step - loss: 0.0563 - acc: 0.9805 - val_loss: 0.0556 - val_acc: 0.9804\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.01-05-0.9745.hdf5\n",
      "val_auc: 0.9745\n",
      "Epoch 6/16\n",
      "143614/143614 [==============================] - 338s 2ms/step - loss: 0.0545 - acc: 0.9809 - val_loss: 0.0559 - val_acc: 0.9795\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.01-06-0.9755.hdf5\n",
      "val_auc: 0.9755\n",
      "Epoch 7/16\n",
      "143614/143614 [==============================] - 341s 2ms/step - loss: 0.0533 - acc: 0.9811 - val_loss: 0.0562 - val_acc: 0.9795\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.01-07-0.9757.hdf5\n",
      "val_auc: 0.9757\n",
      "Epoch 8/16\n",
      "143614/143614 [==============================] - 336s 2ms/step - loss: 0.0523 - acc: 0.9815 - val_loss: 0.0552 - val_acc: 0.9801\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.01-08-0.9765.hdf5\n",
      "val_auc: 0.9765\n",
      "Epoch 9/16\n",
      "143614/143614 [==============================] - 340s 2ms/step - loss: 0.0508 - acc: 0.9818 - val_loss: 0.0570 - val_acc: 0.9792\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "val_auc: 0.9762\n",
      "Epoch 10/16\n",
      "143614/143614 [==============================] - 336s 2ms/step - loss: 0.0504 - acc: 0.9820 - val_loss: 0.0556 - val_acc: 0.9800\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.01-10-0.9770.hdf5\n",
      "val_auc: 0.9770\n",
      "Epoch 11/16\n",
      "143614/143614 [==============================] - 340s 2ms/step - loss: 0.0495 - acc: 0.9823 - val_loss: 0.0566 - val_acc: 0.9792\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "val_auc: 0.9762\n",
      "Epoch 12/16\n",
      "143614/143614 [==============================] - 336s 2ms/step - loss: 0.0487 - acc: 0.9824 - val_loss: 0.0569 - val_acc: 0.9793\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "val_auc: 0.9761\n",
      "Epoch 13/16\n",
      "143614/143614 [==============================] - 341s 2ms/step - loss: 0.0479 - acc: 0.9826 - val_loss: 0.0557 - val_acc: 0.9802\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.01-13-0.9770.hdf5\n",
      "val_auc: 0.9770\n",
      "Epoch 14/16\n",
      "143614/143614 [==============================] - 336s 2ms/step - loss: 0.0472 - acc: 0.9828 - val_loss: 0.0556 - val_acc: 0.9796\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "val_auc: 0.9769\n",
      "Epoch 15/16\n",
      "143614/143614 [==============================] - 340s 2ms/step - loss: 0.0466 - acc: 0.9830 - val_loss: 0.0559 - val_acc: 0.9795\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "val_auc: 0.9768\n",
      "Epoch 16/16\n",
      "143614/143614 [==============================] - 338s 2ms/step - loss: 0.0461 - acc: 0.9832 - val_loss: 0.0569 - val_acc: 0.9793\n",
      "15957/15957 [==============================] - 64s 4ms/step\n",
      "val_auc: 0.9769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/16\n",
      "143614/143614 [==============================] - 342s 2ms/step - loss: 0.1002 - acc: 0.9719 - val_loss: 0.0653 - val_acc: 0.9779\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.02-01-0.9556.hdf5\n",
      "val_auc: 0.9556\n",
      "Epoch 2/16\n",
      "143614/143614 [==============================] - 339s 2ms/step - loss: 0.0664 - acc: 0.9780 - val_loss: 0.0625 - val_acc: 0.9783\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.02-02-0.9670.hdf5\n",
      "val_auc: 0.9670\n",
      "Epoch 3/16\n",
      "143614/143614 [==============================] - 337s 2ms/step - loss: 0.0616 - acc: 0.9790 - val_loss: 0.0579 - val_acc: 0.9800\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.02-03-0.9721.hdf5\n",
      "val_auc: 0.9721\n",
      "Epoch 4/16\n",
      "143614/143614 [==============================] - 340s 2ms/step - loss: 0.0591 - acc: 0.9796 - val_loss: 0.0559 - val_acc: 0.9806\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.02-04-0.9744.hdf5\n",
      "val_auc: 0.9744\n",
      "Epoch 5/16\n",
      "143614/143614 [==============================] - 336s 2ms/step - loss: 0.0570 - acc: 0.9801 - val_loss: 0.0559 - val_acc: 0.9805\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.02-05-0.9761.hdf5\n",
      "val_auc: 0.9761\n",
      "Epoch 6/16\n",
      "143614/143614 [==============================] - 339s 2ms/step - loss: 0.0554 - acc: 0.9805 - val_loss: 0.0553 - val_acc: 0.9802\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.02-06-0.9771.hdf5\n",
      "val_auc: 0.9771\n",
      "Epoch 7/16\n",
      "143614/143614 [==============================] - 336s 2ms/step - loss: 0.0543 - acc: 0.9809 - val_loss: 0.0545 - val_acc: 0.9805\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.02-07-0.9780.hdf5\n",
      "val_auc: 0.9780\n",
      "Epoch 8/16\n",
      "143614/143614 [==============================] - 340s 2ms/step - loss: 0.0527 - acc: 0.9812 - val_loss: 0.0549 - val_acc: 0.9799\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.02-08-0.9784.hdf5\n",
      "val_auc: 0.9784\n",
      "Epoch 9/16\n",
      "143614/143614 [==============================] - 335s 2ms/step - loss: 0.0516 - acc: 0.9815 - val_loss: 0.0567 - val_acc: 0.9791\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.02-09-0.9784.hdf5\n",
      "val_auc: 0.9784\n",
      "Epoch 10/16\n",
      "143614/143614 [==============================] - 340s 2ms/step - loss: 0.0505 - acc: 0.9819 - val_loss: 0.0537 - val_acc: 0.9808\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "val_auc: 0.9781\n",
      "Epoch 11/16\n",
      "143614/143614 [==============================] - 336s 2ms/step - loss: 0.0502 - acc: 0.9818 - val_loss: 0.0554 - val_acc: 0.9797\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "val_auc: 0.9783\n",
      "Epoch 12/16\n",
      "143614/143614 [==============================] - 340s 2ms/step - loss: 0.0491 - acc: 0.9822 - val_loss: 0.0536 - val_acc: 0.9808\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.02-12-0.9785.hdf5\n",
      "val_auc: 0.9785\n",
      "Epoch 13/16\n",
      "143614/143614 [==============================] - 336s 2ms/step - loss: 0.0485 - acc: 0.9823 - val_loss: 0.0547 - val_acc: 0.9802\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "val_auc: 0.9785\n",
      "Epoch 14/16\n",
      "143614/143614 [==============================] - 339s 2ms/step - loss: 0.0479 - acc: 0.9826 - val_loss: 0.0542 - val_acc: 0.9802\n",
      "15957/15957 [==============================] - 62s 4ms/step\n",
      "saving bpe.weights.02-14-0.9786.hdf5\n",
      "val_auc: 0.9786\n",
      "Epoch 15/16\n",
      "143614/143614 [==============================] - 338s 2ms/step - loss: 0.0470 - acc: 0.9829 - val_loss: 0.0538 - val_acc: 0.9807\n",
      "15957/15957 [==============================] - 63s 4ms/step\n",
      "saving bpe.weights.02-15-0.9787.hdf5\n",
      "val_auc: 0.9787\n",
      "Epoch 16/16\n",
      "143614/143614 [==============================] - 337s 2ms/step - loss: 0.0462 - acc: 0.9831 - val_loss: 0.0547 - val_acc: 0.9807\n",
      "15957/15957 [==============================] - 62s 4ms/step\n",
      "val_auc: 0.9787\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/16\n",
      "142080/143614 [============================>.] - ETA: 3s - loss: 0.0983 - acc: 0.9717"
     ]
    }
   ],
   "source": [
    "batch_size = 256  # 32  # 128  # 1024 lowered AUC score even when tried continued training with bigger batch size after small batch size, as well as starting with big batch size and then continuing with smaller size\n",
    "epochs = 16\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "fold = 0\n",
    "for train_index, val_index in kf.split(X_train_padded_bpe, y_train):\n",
    "    checkpoint = ModelCheckpoint(\"weights.{epoch:2d}-{val_loss:.4f}.hdf5\", \n",
    "                                 verbose=1, \n",
    "                                 # save_best_only=True, \n",
    "                                 save_weights_only=True)\n",
    "    lr_reduction = ReduceLROnPlateau(patience=1, verbose=1)\n",
    "    \n",
    "    X_train_split, X_val_split = X_train_padded_bpe[train_index], X_train_padded_bpe[val_index]\n",
    "    y_train_split, y_val_split = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    auc = RocAucEvaluation(validation_data=(X_val_split, y_val_split), cross_validation_fold=fold)\n",
    "    model = build_model(max_sequence_length, word_count, embed_size, embeddings)\n",
    "    history = model.fit(X_train_split, y_train_split, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    # validation_split=0.0,\n",
    "                    validation_data=(X_val_split, y_val_split),\n",
    "                    callbacks=[auc, \n",
    "                               # checkpoint, \n",
    "                               # lr_reduction\n",
    "                              ], \n",
    "                    verbose=1)\n",
    "    del model\n",
    "    K.clear_session()\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/16\n",
      "151592/151592 [==============================] - 1007s 7ms/step - loss: 0.0633 - acc: 0.9783 - val_loss: 0.0549 - val_acc: 0.9783\n",
      "7979/7979 [==============================] - 84s 10ms/step\n",
      "saving weights.01-0.9798.hdf5\n",
      "val_auc: 0.9798\n",
      "Epoch 2/16\n",
      "151592/151592 [==============================] - 996s 7ms/step - loss: 0.0451 - acc: 0.9827 - val_loss: 0.0408 - val_acc: 0.9838\n",
      "7979/7979 [==============================] - 83s 10ms/step\n",
      "saving weights.02-0.9856.hdf5\n",
      "val_auc: 0.9856\n",
      "Epoch 3/16\n",
      "151592/151592 [==============================] - 1002s 7ms/step - loss: 0.0426 - acc: 0.9834 - val_loss: 0.0399 - val_acc: 0.9845\n",
      "7979/7979 [==============================] - 82s 10ms/step\n",
      "saving weights.03-0.9887.hdf5\n",
      "val_auc: 0.9887\n",
      "Epoch 4/16\n",
      "151592/151592 [==============================] - 999s 7ms/step - loss: 0.0408 - acc: 0.9839 - val_loss: 0.0401 - val_acc: 0.9840\n",
      "7979/7979 [==============================] - 81s 10ms/step\n",
      "saving weights.04-0.9898.hdf5\n",
      "val_auc: 0.9898\n",
      "Epoch 5/16\n",
      "151592/151592 [==============================] - 997s 7ms/step - loss: 0.0393 - acc: 0.9844 - val_loss: 0.0377 - val_acc: 0.9847\n",
      "7979/7979 [==============================] - 83s 10ms/step\n",
      "saving weights.05-0.9904.hdf5\n",
      "val_auc: 0.9904\n",
      "Epoch 6/16\n",
      "151592/151592 [==============================] - 1001s 7ms/step - loss: 0.0383 - acc: 0.9848 - val_loss: 0.0389 - val_acc: 0.9848\n",
      "7979/7979 [==============================] - 82s 10ms/step\n",
      "val_auc: 0.9901\n",
      "Epoch 7/16\n",
      "151592/151592 [==============================] - 998s 7ms/step - loss: 0.0377 - acc: 0.9849 - val_loss: 0.0388 - val_acc: 0.9846\n",
      "7979/7979 [==============================] - 82s 10ms/step\n",
      "val_auc: 0.9903\n",
      "Epoch 8/16\n",
      "151592/151592 [==============================] - 997s 7ms/step - loss: 0.0370 - acc: 0.9851 - val_loss: 0.0390 - val_acc: 0.9840\n",
      "7979/7979 [==============================] - 82s 10ms/step\n",
      "saving weights.08-0.9904.hdf5\n",
      "val_auc: 0.9904\n",
      "Epoch 9/16\n",
      "151592/151592 [==============================] - 996s 7ms/step - loss: 0.0360 - acc: 0.9856 - val_loss: 0.0404 - val_acc: 0.9829\n",
      "7979/7979 [==============================] - 82s 10ms/step\n",
      "val_auc: 0.9903\n",
      "Epoch 10/16\n",
      "151592/151592 [==============================] - 1005s 7ms/step - loss: 0.0354 - acc: 0.9858 - val_loss: 0.0400 - val_acc: 0.9834\n",
      "7979/7979 [==============================] - 83s 10ms/step\n",
      "val_auc: 0.9903\n",
      "Epoch 11/16\n",
      "151592/151592 [==============================] - 996s 7ms/step - loss: 0.0345 - acc: 0.9862 - val_loss: 0.0424 - val_acc: 0.9828\n",
      "7979/7979 [==============================] - 82s 10ms/step\n",
      "val_auc: 0.9904\n",
      "Epoch 12/16\n",
      "151592/151592 [==============================] - 997s 7ms/step - loss: 0.0338 - acc: 0.9863 - val_loss: 0.0394 - val_acc: 0.9840\n",
      "7979/7979 [==============================] - 82s 10ms/step\n",
      "saving weights.12-0.9905.hdf5\n",
      "val_auc: 0.9905\n",
      "Epoch 13/16\n",
      "151592/151592 [==============================] - 1000s 7ms/step - loss: 0.0333 - acc: 0.9865 - val_loss: 0.0407 - val_acc: 0.9836\n",
      "7979/7979 [==============================] - 82s 10ms/step\n",
      "saving weights.13-0.9905.hdf5\n",
      "val_auc: 0.9905\n",
      "Epoch 14/16\n",
      "151592/151592 [==============================] - 997s 7ms/step - loss: 0.0326 - acc: 0.9868 - val_loss: 0.0412 - val_acc: 0.9836\n",
      "7979/7979 [==============================] - 82s 10ms/step\n",
      "val_auc: 0.9904\n",
      "Epoch 15/16\n",
      "151592/151592 [==============================] - 995s 7ms/step - loss: 0.0326 - acc: 0.9868 - val_loss: 0.0413 - val_acc: 0.9837\n",
      "7979/7979 [==============================] - 82s 10ms/step\n",
      "val_auc: 0.9902\n",
      "Epoch 16/16\n",
      "151592/151592 [==============================] - 999s 7ms/step - loss: 0.0317 - acc: 0.9871 - val_loss: 0.0412 - val_acc: 0.9835\n",
      "7979/7979 [==============================] - 82s 10ms/step\n",
      "val_auc: 0.9899\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"weights.15-0.9905.hdf5\")  # capsule network baseline\n",
    "# model.load_weights(\"weights.10-0.9904.hdf5\")  # rnn size increased from 128 to max sequence length\n",
    "model.load_weights(\"weights.08-0.9906.hdf5\")  # added numberbatch with restored rnn size to 128\n",
    "# model.load_weights(\"weights.13-0.9905.hdf5\")  # added glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.3 s, sys: 6.08 s, total: 1min\n",
      "Wall time: 59.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "y_pred = model.predict(X_test_padded, batch_size=1024)\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Toxic Comment Classification Challenge"
     ]
    }
   ],
   "source": [
    "! kaggle competitions submit -c jigsaw-toxic-comment-classification-challenge -f submission.csv -m \"added numberbatch embeddings branch\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
