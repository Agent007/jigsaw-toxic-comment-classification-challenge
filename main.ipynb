{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "#train = train.sample(frac=0.05)  # 157975 original total, so let's prototype models with a fraction of that\n",
    "validation_fraction = 0.01  # change to 1% for training on complete training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "max_features = None  # 30000\n",
    "maxlen = 128   # doubling to 256 worsened validation AUC score  # 100\n",
    "embed_size = 300  # should match embedding file\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "EMBEDDING_FILE = 'crawl-300d-2M.vec'  # fasttext\n",
    "\n",
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(get_coefs(*file.rstrip().rsplit(' ')) for file in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index  # len(word_index) == 394787\n",
    "nb_words = len(word_index) #min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    # if i >= max_features: \n",
    "    #     break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Bidirectional, concatenate, CuDNNGRU, Dense, Embedding, Input, GlobalAveragePooling1D, GlobalMaxPooling1D, GRU, SpatialDropout1D\n",
    "# from keras.regularizers import l2\n",
    "# from keras.constraints import maxnorm\n",
    "\n",
    "# from qrnn import QRNN\n",
    "\n",
    "def build_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, embed_size, weights=[embedding_matrix])(inp)\n",
    "    # x = SpatialDropout1D(0.1)(x)\n",
    "    rnn_size = maxlen\n",
    "    x = Bidirectional(CuDNNGRU(rnn_size, return_sequences=True))(x)\n",
    "    # x = Bidirectional(CuDNNGRU(rnn_size, return_sequences=True))(x)  # 2nd bidirectional layer didn't help with training subsample\n",
    "    # x = QRNN(rnn_size, window_size=7, return_sequences=True)(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    multiclass_label_count = 6\n",
    "    outp = Dense(multiclass_label_count, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='nadam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=validation_fraction)\n",
    "\n",
    "\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch + 1, score))\n",
    "\n",
    "            \n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "checkpoint = ModelCheckpoint(\"weights.{epoch:02d}-{val_loss:.4f}.hdf5\", \n",
    "                             verbose=1, \n",
    "                             # save_best_only=True, \n",
    "                             save_weights_only=True)\n",
    "lr_reduction = ReduceLROnPlateau(patience=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:94: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 118436100 elements. This may consume a large amount of memory.\n",
      "  \"This may consume a large amount of memory.\" % num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 157975 samples, validate on 1596 samples\n",
      "Epoch 1/8\n",
      "157975/157975 [==============================] - 848s 5ms/step - loss: 0.0466 - acc: 0.9825 - val_loss: 0.0484 - val_acc: 0.9815\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.987201 \n",
      "\n",
      "\n",
      "Epoch 00001: saving model to weights.01-0.0484.hdf5\n",
      "Epoch 2/8\n",
      "157975/157975 [==============================] - 846s 5ms/step - loss: 0.0347 - acc: 0.9862 - val_loss: 0.0524 - val_acc: 0.9799\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.987098 \n",
      "\n",
      "\n",
      "Epoch 00002: saving model to weights.02-0.0524.hdf5\n",
      "Epoch 3/8\n",
      "157975/157975 [==============================] - 846s 5ms/step - loss: 0.0273 - acc: 0.9893 - val_loss: 0.0540 - val_acc: 0.9796\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.984244 \n",
      "\n",
      "\n",
      "Epoch 00003: saving model to weights.03-0.0540.hdf5\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 4/8\n",
      "157975/157975 [==============================] - 846s 5ms/step - loss: 0.0165 - acc: 0.9937 - val_loss: 0.0575 - val_acc: 0.9794\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.984149 \n",
      "\n",
      "\n",
      "Epoch 00004: saving model to weights.04-0.0575.hdf5\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "Epoch 5/8\n",
      "157975/157975 [==============================] - 846s 5ms/step - loss: 0.0114 - acc: 0.9961 - val_loss: 0.0588 - val_acc: 0.9795\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.984227 \n",
      "\n",
      "\n",
      "Epoch 00005: saving model to weights.05-0.0588.hdf5\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.0000001313746906e-06.\n",
      "Epoch 6/8\n",
      "157975/157975 [==============================] - 846s 5ms/step - loss: 0.0109 - acc: 0.9963 - val_loss: 0.0590 - val_acc: 0.9794\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.984221 \n",
      "\n",
      "\n",
      "Epoch 00006: saving model to weights.06-0.0590.hdf5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 2.000000222324161e-07.\n",
      "Epoch 7/8\n",
      "157975/157975 [==============================] - 846s 5ms/step - loss: 0.0108 - acc: 0.9963 - val_loss: 0.0590 - val_acc: 0.9793\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.984213 \n",
      "\n",
      "\n",
      "Epoch 00007: saving model to weights.07-0.0590.hdf5\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-08.\n",
      "Epoch 8/8\n",
      "157975/157975 [==============================] - 846s 5ms/step - loss: 0.0108 - acc: 0.9963 - val_loss: 0.0590 - val_acc: 0.9793\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.984213 \n",
      "\n",
      "\n",
      "Epoch 00008: saving model to weights.08-0.0590.hdf5\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 2.000000165480742e-09.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16  # 1024 lowered AUC score even when tried continued training with bigger batch size after small batch size, as well as starting with big batch size and then continuing with smaller size\n",
    "epochs = 8\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[RocAuc, \n",
    "                               checkpoint, \n",
    "                               lr_reduction], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.01-0.0484.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Toxic Comment Classification Challenge"
     ]
    }
   ],
   "source": [
    "! kaggle competitions submit -c jigsaw-toxic-comment-classification-challenge -f submission.csv -m \"initial baseline with max embedding matrix\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
